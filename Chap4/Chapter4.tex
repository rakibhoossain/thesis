\chapter{System Implementation}
\label{chap:Implementation}

\section{Introduction}
This chapter provides a granular view of the software engineering principles and code-level implementation details of the developed system. Having established the theoretical methodology in Chapter 3, we now focus on the practical realization of the system, including the development environment, software architecture, core modules, and the user-facing web application.

\section{The Engineering Stack}
Building a deep learning system isn't just about math; it's about managing resources. My development environment was dictated by one major constraint: \textbf{Hardware}.

\subsection{Hardware Constraints}
Training a transformer model requires massive parallel computation. I utilized Google Colab's cloud environment, specifically the **NVIDIA T4 GPU**.
\begin{itemize}
    \item \textbf{Why not CPU?} A standard laptop CPU would take approximately 40 minutes per epoch. The T4 reduced this to roughly 3 minutes.
    \item \textbf{Memory Hacks:} The T4 has 16GB of VRAM. With a batch size of 8 and a 512-token sequence, I constantly hit "Out of Memory" (OOM) errors. To fix this, I enabled **Mixed Precision (FP16)**. This treats the weights as 16-bit floats instead of 32-bit, effectively halving the memory usage without losing significant accuracy.
\end{itemize}

\subsection{Software dependencies}
I avoided "reinventing the wheel" by leaning on established libraries:
\begin{itemize}
    \item **Hugging Face Transformers:** Writing a Self-Attention layer from scratch is error-prone. I used the `Transformers` library to load the pre-trained weights.
    \item **PyTorch:** I chose PyTorch over TensorFlow because of its dynamic computation graph, which made debugging the custom classification head much easier.
    \item **Gradio:** For the frontend, I needed something that could talk to Python natively. Gradio allowed me to build the UI in pure Python, skipping the need for React or HTML/CSS.
\end{itemize}

\section{System Architecture}
The system isn't a single script; it's a pipeline of three distinct components communicating via disk storage.

\begin{figure}[h]
    \centering
    \fbox{
    \begin{minipage}{0.9\textwidth}
        \centering
        \textbf{Stage 1: Ingestion} \\ 
        Raw Web Data $\rightarrow$ [Cleaner.py] $\rightarrow$ Clean CSV \\
        $\downarrow$ \\
        \textbf{Stage 2: Training} \\
        Clean CSV $\rightarrow$ [FineTuner Class] $\rightarrow$ \texttt{model.pt} (Weights) \\
        $\downarrow$ \\
        \textbf{Stage 3: Inference} \\
        User Text $\rightarrow$ [Analyzer.py] $\rightarrow$ JSON/UI visualization
    \end{minipage}
    }
    \caption{The Three-Stage Data Pipeline}
    \label{fig:arch}
\end{figure}

\subsection{The Training Module}
The heart of the research is \texttt{fine\_tune.py}. I encapsulated the logic in a `BanglaSentimentFineTuner` class to keep the notebook clean.
Key implementation details:
\begin{itemize}
    \item \textbf{Handling Crashes:} Initially, the training crashed because some "text" fields in the CSV were empty strings (NaN). I added a validation step in `prepare_dataset()` to drop any row where the text length was $<5$ characters.
    \item \textbf{Stratification:} I used `sklearn`'s stratified split. This was critical. Without it, one random test set might have 90\% negative news, giving me a false sense of accuracy.
    \item \textbf{Metrics:} Accuracy wasn't enough. I implemented a custom `compute_metrics` function to track the **Weighted F1-Score**, which tells me if the model is ignoring the smaller classes (like "Neutral").
\end{itemize}

\subsection{The Inference Engine}
Once the model is trained, we need to use it. The `analyzer.py` script loads the saved weights (`model.pt`).
The biggest challenge here was **speed**. Running the model one article at a time is fine for a demo, but slow for a dataset. I implemented **Batch Processing** using Hugging Face's `pipeline` feature, which feeds 16 articles to the GPU at once. I also added a `tqdm` progress bar so the user isn't staring at a frozen screen.

\section{The "No-Code" Web Interface}
A command-line tool is intimidating. To make this accessible to a journalist or social scientist, I built a web app.

\subsection{Why Gradio?}
I chose Gradio because it allows "Hot Reloading" during development. I designed two specific tabs:
\begin{enumerate}
    \item **The "Quick Check" Tab:** This is a simple text box. You paste a headline, and it gives you a red/green bar chart. It solves the use case of "I just read this one article, is it biased?"
    \item **The "Batch" Tab:** This solves the research use case. A user can upload a CSV of 5,000 comments. The backend processes them in batches, calculates the aggregate sentiment, and generates visualizations (Pie Charts/Time Series) automatically using Matplotlib.
\end{enumerate}

\subsection{Error Handling in the Wild}
Real users upload messy data. I wrapped the processing loop in a `try-except` block. If Row \#4,032 is corrupted, the system logs the error ("Skipped malformed row") and continues to Row \#4,033, rather than crashing the entire server. This resilience is vital for a production-ready tool.
