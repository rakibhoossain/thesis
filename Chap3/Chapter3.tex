\chapter{Methodology}
\label{chap:Methodology}

\section{Introduction}
This chapter presents a detailed exposition of the research methodology employed to develop the Bengali News Sentiment Analysis system. The proposed framework is a multi-stage pipeline designed to handle the complexities of low-resource language processing. The methodology is divided into three primary phases: (1) Automated Data Acquisition and Curation, (2) Dataset Preprocessing and Weak Supervision, and (3) Transfer Learning and Fine-tuning. Figure \ref{fig:methodology_overview} provides a high-level visual representation of the entire workflow.

\section{Phase 1: Building the Dataset}
The biggest hurdle in Bengali NLP isn't model architecture; it's the lack of data. When I started, I couldn't find a single open dataset of formal news articles that was large enough (>50k samples) for deep learning. So, I had to build one myself.

\subsection{Targeting the Right Sources}
I needed data that represented standard, formal Bengali (Cholitobhasha). I chose four outlets not just for volume, but for linguistic variety:
\begin{itemize}
    \item \textbf{Prothom Alo:} The "Gold Standard" for grammar. If the model learns from here, it learns correct syntax.
    \item \textbf{BBC Bangla:} Crucial for learning "Neutral" sentiment. International broadcasters tend to avoid the emotive adjectives found in local crime reporting.
    \item \textbf{Kalerkantho \& Jugantor:} These provided the "noise." They often use more sensationalist language, which pushes the model to distinguish between genuine negativity and mere sensationalism.
\end{itemize}

\subsection{The Scraping Engine}
I initially tried using \texttt{Selenium}, but it was too slow for fetching 100,000+ pages. I switched to a lightweight mix of \texttt{requests} and \texttt{BeautifulSoup}.

The real challenge wasn't downloading HTML; it was traversing the pagination logic. Most sites use a simple query parameter (e.g., `?page=40`), but they often throw "403 Forbidden" errors if you scrape too fast. To bypass this, I implemented a "Politeness Policy"—injecting a random sleep interval (between 1 and 3 seconds) between requests to mimic human browsing.

\begin{algorithm}
\caption{The Scraping Logic}
\label{alg:scraper}
\begin{algorithmic}[1]
\State \textbf{Input:} Seed URLs $U$, Target Count $N \approx 100,000$
\State \textbf{Output:} Cleaned Text Corpus $D$
\State $D \gets \emptyset$
\For{$url$ in $U$}
    \While{$len(D) < N$}
        \State $page \gets \text{Fetch}(url)$
        \If{$page.status == 403$} 
            \State \textbf{Sleep}(60s) \Comment{Cooldown on block}
            \textbf{continue} 
        \EndIf
        \State $articles \gets \text{ExtractLinks}(page)$
        \For{$link$ in $articles$}
            \State $raw\_html \gets \text{Fetch}(link)$
            \State $text \gets \text{Parse}(raw\_html, \text{'div.story-content'})$
            \State $D.append(text)$
            \State \textbf{Sleep}(Random(1, 3)) \Comment{Politeness Delay}
        \EndFor
    \EndWhile
\EndFor
\State \Return $D$
\end{algorithmic}
\end{algorithm}

\section{Phase 2: Cleaning the Mess}
Raw web data is dirty. My initial scrape contained 110,000 articles, but nearly 40\% were unusable.

\subsection{The Unicode Nightmare}
Bengali text is plagued by Unicode inconsistencies. The main culprit is the "Zero Width Joiner" and inconsistent vowel representations. For instance, the letter "o" can be written as a single character or two separate diacritics. 
To fix this, I applied **NFKC Normalization**. This forces all characters into their "canonical" composed forms, ensuring that "boy" written in two different ways is treated as the same word by the model.

\subsection{Deduplication}
I noticed a severe data leakage issue: when a major event happens (e.g., a budget release), multiple newspapers publish the exact same wire report from BSS (national news agency). If I kept these, my model would overfit on these specific phrases.
I used MD5 hashing on the article bodies to identify and remove 9,500+ exact duplicates.

\subsection{Final Stats}
After cleaning, I was left with a high-quality corpus of \textbf{70,300 articles}, containing roughly 18.5 million tokens. This is significantly larger than previous social-media based datasets.

\section{Phase 3: The Labeling Bottleneck}
I had 70,000 clean articles, but I had 0 labels. Manually reading each one would take an estimated 3,000 hours. Since I lacked the budget for paid annotators, I designed a **Semi-Supervised** pipeline.

\subsection{Using a "Teacher" Model}
I used a large, pre-trained multilingual model as a "Teacher" to generate initial weak labels. Think of this as getting a rough draft. The teacher reads the article and guesses if it's Positive, Negative, or Neutral.
I chose a 5-class pre-trained model but quickly found it was too granular. It often confused "Very Positive" with "Positive." To fix this, I flattened the output into 3 distinct classes using a custom mapping:
\begin{equation}
    M(y) = 
    \begin{cases} 
      \text{Negative} & \text{if score} < 2 \\
      \text{Positive} & \text{if score} > 4 \\
      \text{Neutral} & \text{otherwise}
   \end{cases}
\end{equation}
This simple trick drastically reduced noise. I then manually spot-checked 500 random samples to verify that the teacher wasn't hallucinating.

\section{Phase 4: Training the Model}
For the actual training, I had to be pragmatic. Large models like BERT-Base (110M parameters) are powerful, but they require massive VRAM. Working within the constraints of a Google Colab T4 GPU (16GB), I chose **DistilBERT**.

\subsection{Why DistilBERT?}
It wasn't just about speed. DistilBERT has 40\% fewer parameters than BERT, which means it fits comfortably in memory even with a batch size of 8 and a sequence length of 512.
\begin{itemize}
    \item \textbf{Input:} The model takes in 512 tokens. If an article was longer, I truncated the end. Why? Because in news writing (the "Inverted Pyramid" style), the most critical information is in the first few paragraphs.
    \item \textbf{Output:} I replaced the original classification head with a simple linear layer mapped to my 3 sentiment classes.
\end{itemize}

\subsection{The Training Recipe}
Training a transformer is fickle. If the Learning Rate is too high, the model forgets everything (Catastrophic Forgetting). If it's too low, it never learns. After several failed runs, I settled on:
\begin{itemize}
    \item \textbf{optimizer:} AdamW (it handles weight decay better than standard Adam).
    \item \textbf{Learning Rate:} $2e-5$. This is extremely low, but necessary for fine-tuning.
    \item \textbf{Epochs:} I set it to 5, but implemented "Early Stopping." If the validation loss didn't drop for 2 epochs, I stopped training to save time.
\end{itemize}

\subsection{Handling Imbalance}
News is rarely "Happy." My dataset was heavily skewed towards Negative news (crime, accidents). If I just trained blindly, the model would learn to guess "Negative" every time and still get high accuracy. To fight this, I used **Stratified Splitting**—ensuring that my validation set had the exact same ratio of Positive/Negative/Neutral examples as the training set, giving me a fair evaluation.
