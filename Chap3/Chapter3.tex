\chapter{Methodology}
\label{chap:Methodology}

\section{Introduction}
This chapter presents a detailed exposition of the research methodology employed to develop the Bengali News Sentiment Analysis system. The proposed framework is a multi-stage pipeline designed to handle the complexities of low-resource language processing. The methodology is divided into three primary phases: (1) Automated Data Acquisition and Curation, (2) Dataset Preprocessing and Weak Supervision, and (3) Transfer Learning and Fine-tuning. Figure \ref{fig:methodology_overview} provides a high-level visual representation of the entire workflow.

\section{Phase 1: Automated Data Acquisition}
Given the scarcity of large-scale open datasets for Bengali news, the first critical step was to construct a robust data ingestion engine. We developed a custom, distributed web scraper tailored to the structural nuances of major Bengali news portals.

\subsection{Target Source Selection}
We strategically selected four major news sources to ensure linguistic and topical diversity:
\begin{itemize}
    \item \textbf{Prothom Alo:} The most widely circulated Bengali daily. It follows a strict standard for "Cholitobhasha" (Standard Colloquial Bengali). Articles from this source provide the baseline for grammatical correctness.
    \item \textbf{Kalerkantho \& Jugantor:} These papers are known for their extensive coverage of local news, crime, and social issues. They often contain slightly more emotive and sensationalist language compared to Prothom Alo.
    \item \textbf{ BBC Bangla:} As an international broadcaster, BBC Bangla maintains a very high standard of neutral, formal reporting. Including this source helps the model learn to identify "Neutral" sentiment accurately, preventing bias towards sensationalism.
\end{itemize}

\subsection{Distributed Scraping Architecture}
The scraping module was implemented using Python's \texttt{requests} library for HTTP handling and \texttt{BeautifulSoup} for DOM parsing. To handle the volume of requests required (fetching 100,000+ pages), we implemented a sequential scraping logic with politeness delays.

\subsubsection{Pagination and Traversal}
Most news sites use a pagination structure (e.g., \texttt{site.com/archive?page=1}). Our algorithm iterates through these pages to extract article URLs. Algorithm \ref{alg:scraper} details this process.

\begin{algorithm}
\caption{News Article Scraping Algorithm}
\label{alg:scraper}
\begin{algorithmic}[1]
\State \textbf{Input:} Seed URL List $U$, Max Depth $N$
\State \textbf{Output:} Raw Dataset $D = \{(headline, body, date, category)\}_{i=1}^M$
\State $D \gets \emptyset$
\State $Visited \gets \emptyset$
\For{$url$ in $U$}
    \For{$page \gets 1$ to $N$}
        \State $response \gets \text{GET}(url + "?page=" + str(page))$
        \If{$response.status \neq 200$} \textbf{continue} \EndIf
        \State $soup \gets \text{ParseHTML}(response.text)$
        \State $links \gets soup.\text{findAll}('a', class_='link-overlay')$
        \For{$link$ in $links$}
            \If{$link$ in $Visited$} \textbf{continue} \EndIf
            \State $article \gets \text{GET}(link)$
            \State $text \gets \text{ExtractText}(article.body, 'div', 'story-element') $
            \If{$\text{Length}(text) > \text{MinLength}$}
                \State $D.append(text)$
                \State $Visited.add(link)$
            \EndIf
            \State $\text{Sleep}(\text{Random}(1.0, 3.0))$ \Comment{Politeness Delay}
        \EndFor
    \EndFor
\EndFor
\State \Return $D$
\end{algorithmic}
\end{algorithm}

\subsubsection{Handling Dynamic Content and Challenges}
Several technical challenges were addressed:
\begin{itemize}
    \item \textbf{Anti-Scraping Mechanisms:} Many sites block rapid requests. We mitigated this by randomizing the \texttt{User-Agent} header and introducing a random sleep interval ($t \sim U(1, 3)$ seconds) between requests.
    \item \textbf{DOM Validity:} News sites are often cluttered with ads and navigation bars. We used specific CSS selectors (e.g., \texttt{div.story-content}) to target only the journalistic content, discarding sidebars and comments sections.
\end{itemize}

\section{Phase 2: Preprocessing and Weak Labeling}
Raw web data is inherently noisy. A rigorous cleaning pipeline was established to ensure data quality.

\subsection{Text Normalization}
Bengali text processing involves unique challenges related to Unicode representation.
\begin{enumerate}
    \item \textbf{Unicode Normalization (NFKC):} Different operating systems may represent the same character differently (e.g., composed vs. decomposed forms). We applied NFKC (Normalization Form Compatibility Composition) to standardize all text.
    \item \textbf{Noise Removal:} We removed HTML tags ($<br>, <p>$), excessive whitespace, and non-Bengali characters (except standard English punctuation used in Bengali).
    \item \textbf{Deduplication:} Duplicate articles are common due to wire services (e.g., BSS). We removed duplicates based on MD5 hashes of the article body.
\end{enumerate}

\subsection{Dataset Statistics}
After cleaning, we conducted a statistical analysis of the dataset to understand its characteristics.
\begin{table}[h]
\centering
\caption{Descriptive Statistics of the Curated Dataset}
\label{tab:stats}
\begin{tabular}{|l|r|}
\hline
\textbf{Metric} & \textbf{Value} \\ \hline
Total Articles & 70,300 \\ \hline
Total Tokens & $\approx$ 18.5 Million \\ \hline
Average Words per Article & 263.4 \\ \hline
Max Words per Article & 2,150 \\ \hline
Vocabulary Size (Unique Words) & 85,420 \\ \hline
\end{tabular}
\end{table}

This high volume of tokens ensures that the model is exposed to a rich variety of syntactic structures and vocabulary usage, which is crucial for training deep neural networks.

\subsection{Semi-Automated Data Annotation}
Creating a robustly labeled dataset for a low-resource language is a significant bottleneck. Manually annotating 70,300 articles is resource-prohibitive. To solve this, we designed a novel semi-supervised annotation pipeline. This approach leveraged the cross-lingual capabilities of a large-scale pre-trained model to assist in labeling, effectively bootstrapping our dataset creation process.

\subsubsection{Automated Labeling Pipeline}
We integrated a \texttt{Pre-trained Multilingual DistilBERT} model into our labeling workflow. This model, having been trained on a massive corpus covering 104 languages, served as a linguistic knowledge base. By utilizing this model, we were able to project high-quality weak labels onto our scraped Bengali corpus. This strategic choice allowed us to transform our raw, scraped data into a valuable supervised learning resource without years of manual labor.

\subsubsection{Label Mapping Logic}
The Teacher model outputs 5 sentiment classes: $C_{teacher} = \{\text{Very Negative, Negative, Neutral, Positive, Very Positive}\}$.
For our specific news domain, a 5-class granularity is often ambiguous and introduces noise. We simplified this to a robust 3-class system $C_{student} = \{\text{Negative, Neutral, Positive}\}$ using the following mapping function $M$:

\begin{equation}
    M(y_{teacher}) = 
    \begin{cases} 
      0 \text{ (Negative)} & \text{if } y_{teacher} \in \{\text{Very Negative, Negative}\} \\
      2 \text{ (Positive)} & \text{if } y_{teacher} \in \{\text{Very Positive, Positive}\} \\
      1 \text{ (Neutral)} & \text{otherwise}
   \end{cases}
\end{equation}

This mapping reduces the "decision boundary" confusion between "Very Positive" and "Positive", leading to a more stable training signal for the Student model.

\section{Phase 3: Fine-Tuning Strategy}
Fine-tuning involves taking a pre-trained language model and updating its weights on a specific downstream task. This utilizes the concept of Transfer Learning.

\subsection{Justification for DistilBERT}
We chose DistilBERT over larger models like BERT-Large or XLM-RoBERTa-Large for three key reasons:
\begin{itemize}
    \item \textbf{Parameter Efficiency:} DistilBERT has only 66 million parameters compared to mBERT's 110 million. This reduces the risk of overfitting on our medium-sized dataset.
    \item \textbf{Inference Latency:} In a real-time web application, response time is critical. DistilBERT provides 60\% faster inference, ensuring a smooth user experience.
    \item \textbf{Green AI:} Training smaller models consumes significantly less energy, aligning with the principles of sustainable AI development.
\end{itemize}

\subsection{Model Architecture}
The architecture consists of:
\begin{itemize}
    \item \textbf{Embeddings:} The input text is tokenized using WordPiece tokenizer and converted to vectors $E \in \mathbb{R}^{L \times 768}$, where $L=512$ is the sequence length.
    \item \textbf{Transformer Layers:} 6 layers of Transformer encoders process the embeddings.
    \item \textbf{Classification Head:} We discarded the pre-training head and added a simple linear classification layer:
    \begin{equation}
        y = \text{Softmax}(W \cdot h_{CLS} + b)
    \end{equation}
    Where $h_{CLS}$ is the hidden state of the special \texttt{[CLS]} token from the last layer, which serves as the aggregate representation of the sentence. $W \in \mathbb{R}^{3 \times 768}$ is the weight matrix.
\end{itemize}

\subsection{Hyperparameter Optimization}
We employed the AdamW optimizer, a variant of Adam that decouples weight decay from the gradient update, which is crucial for training Transformers. The update rule is:
\begin{equation}
    \theta_{t} = \theta_{t-1} - \eta (\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1})
\end{equation}
Where $\lambda$ is the weight decay coefficient. 

Table \ref{tab:hyperparams} summarizes the optimal hyperparameters found via grid search.

\begin{table}[h]
\centering
\caption{Optimal Hyperparameter Configuration}
\label{tab:hyperparams}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Justification} \\ \hline
Learning Rate & $2 \times 10^{-5}$ & Low LR to prevent catastrophic forgetting \\ \hline
Batch Size & 8 & Limited by GPU VRAM (for 512 tokens) \\ \hline
Epochs & 5 & Selected via Early Stopping \\ \hline
Weight Decay & 0.01 & Regularization to prevent overfitting \\ \hline
Warmup Steps & 200 & Gradual tailored LR increase \\ \hline
\end{tabular}
\end{table}

\subsection{Loss Function}
We minimized the Cross-Entropy Loss, which is standard for multi-class classification:
\begin{equation}
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{3} y_{i,c} \log(\hat{y}_{i,c})
\end{equation}
Where $y_{i,c}$ is the binary indicator (0 or 1) if class label $c$ is the correct classification for observation $i$, and $\hat{y}_{i,c}$ is the predicted probability.

\subsection{Stratified Splitting}
News datasets are inherently imbalanced (Negative news dominates). To ensure our validation, set is representative, we used Stratified Sampling for the Train-Test split. This ensures that the proportion of Positive, Negative, and Neutral samples remains constant across both training and testing sets, preventing the model from just learning the prior distribution of the dominant class.
