\chapter{Introduction}
\label{chap:Intro}

\section{Background and Motivation}
We live in an era where news travels faster than it can be verified. For a country like Bangladesh, with over 180 million people, the digital news ecosystem is chaotic, vibrant, and overwhelming. Every day, thousands of articles are published online—ranging from political shifting tides to local economic crises. Manually tracking this deluge of information is no longer possible. 

I undertook this research because I saw a disconnect. While English NLP models like GPT-4 can detect subtle sarcasm in a New York Times op-ed, Bengali tools often fail to distinguish between a political critique and a simple report. This gap is not just academic; it has real-world consequences. A sudden spike in negative news about commodity prices, for instance, serves as an early warning for economic instability, but only if we can detect it in real-time.

My motivation stems from the linguistic "messiness" of Bengali. It is a language where a single verb root like ``কর'' (do) can morph into ``করেছিলাম'' (I did), ``করছি'' (doing), or ``করানো'' (causing to do). Most existing tools, built for clearer syntax like English, choke on this morphology. I wanted to build a system that doesn't just translate Bengali to English to understand it, but one that natively \textit{reads} Bengali news with all its complex structural glory.

\section{Problem Statement}
The core problem is simple: \textbf{we lack a reliable way to automate the reading of formal Bengali news.}

When I started looking for solutions, I hit three specific walls:
\begin{enumerate}
    \item \textbf{The Translation Trap:} The most common advice I found was ``Just translate it to English and use VADER.'' This fails miserably. A phrase like ``ঘোড়ার ডিম'' (literally: horse's egg) translates to an absurdity in English, but to a Bengali speaker, it clearly means ``nonsense'' or ``nothing.'' Translation destroys these local idioms.
    \item \textbf{The Data Desert:} While there are datasets for Bengali, they are mostly scraped from social media comments (Facebook/YouTube). Training a model on YouTube comments to read a formal newspaper editorial is like teaching a child slang and expecting them to understand a legal contract. The grammar, vocabulary, and tone are completely different.
    \item \textbf{Dictionary Limitations:} Old-school methods relying on lists of ``good'' and ``bad'' words fail when context shifts. The word ``জটিল'' can mean ``complicated'' (negative) in a math problem, but ``awesome'' (positive) in a movie review. Static dictionaries cannot track this shift.
\end{enumerate}

I realized that to solve this, I couldn't rely on generic APIs. I needed a dedicated pipeline that could ingest full-length articles, handle the specific dialect of Bengali newspapers (Sadhu/Cholitobhasha mix), and output a reliable sentiment score.

\section{Research Goal and Approach}
My goal was straightforward but technically demanding: \textbf{create a system that can read a Bengali news article and tell me if it's good news or bad news with >90\% accuracy.}

To get there, I had to break the project into tangible steps:
\begin{enumerate}
    \item \textbf{Scraping the Web:} I needed data. Not just a few hundred rows, but thousands. I built a scraper to target five major news outlets (like Prothom Alo and bdnews24), handling their different HTML structures and anti-bot protections.
    \item \textbf{Cleaning the Mess:} Real-world data is dirty. I spent significant time normalizing Unicode (fixing broken characters) and deduplicating articles that were reposted across different days.
    \item \textbf{Teaching the Model:} I chose to fine-tune a \texttt{DistilBERT} model. Why DistilBERT? Because it's lighter and faster than the colossal standard BERT, making it possible to run on the limited GPU resources I had available (Google Colab T4). I wanted a solution that was practical, not just theoretical.
    \item \textbf{Building the Tool:} Finally, a model in a notebook is useless to a journalist. I wrapped the model in a Gradio web interface so that anyone could paste a URL and get an instant analysis.
\end{enumerate}

\section{Scope and Constraints}
\subsection{What I Focused On}
This research strictly looks at **text**. I ignored images or videos embedded in the articles. Also, I focused on **formal Bengali** (the kind used in newspapers). The slang you see on TikTok or in casual Messenger chats is a different beast entirely, and my model isn't trained for that.

\subsection{Where I Hit Limits}
\begin{itemize}
    \item \textbf{Token Limits:} The biggest technical headache was the 512-token limit of BERT models. Long editorials often got cut off. I incorporated a truncation strategy (taking the first 512 tokens), which works for news (where the lead paragraph has the most info) but isn't perfect.
    \item \textbf{Labeling Reliance:} I couldn't manually read and label 70,000 articles—it would have taken years. I used a "teacher model" (a pre-trained sentiment tool) to generate initial labels, which I then spot-checked. This "semi-supervised" approach saved time but carries the risk that the student model mimics the teacher's mistakes.
\end{itemize}

\section{Contributions}
If this thesis leaves a mark, I hope it is through these three contributions:
\begin{enumerate}
    \item **The Dataset:** I am releasing the cleaned dataset of 70,300+ articles. To my knowledge, this is one of the largest open collections of formal Bengali news text available for research.
    \item **The Fine-Tuned Model:** My fine-tuned DistilBERT model achieved a 93\% accuracy in testing, proving that even "smaller" transformer models can punch above their weight if the data is clean.
    \item **The "No-Code" Interface:** The accompanying web app means a user doesn't need to know Python to use this tool.
\end{enumerate}

\section{Roadmap}
The rest of this thesis tracks my journey in building this system:
\begin{itemize}
    \item \textbf{Chapter 2 (Literature Review)}: I dig into the history of sentiment analysis, looking at how we moved from simple dictionary lookups to the modern "Transformer" revolution, and why Bengali research has often lagged behind.
    \item \textbf{Chapter 3 (Methodology)}: Here I open up the "black box." I explain the mechanics of the Transformer's self-attention (how the model learns context) and detail exactly how I scraped and cleaned the 70,000 articles.
    \item \textbf{Chapter 4 (Implementation)}: This is the engineering chapter. I walk through the code, the server setup, and the specific libraries (like Hugging Face) that power the analysis.
    \item \textbf{Chapter 5 (Results)}: The moment of truth. I share the accuracy scores, the confusion matrices, and—most importantly—where the model failed and why.
    \item \textbf{Chapter 6 (Conclusion)}: I wrap up by discussing what I would do differently next time, specifically looking at handling longer articles and detecting sarcasm.
\end{itemize}
