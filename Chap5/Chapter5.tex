\chapter{Results and Critical Analysis}
\label{chap:Results}

\section{Introduction}
This chapter presents a comprehensive evaluation of the performance of the proposed system. We utilize standard statistical metrics to quantify the model's accuracy, precision, recall, and F1-score. Furthermore, we conduct a comparative analysis against existing state-of-the-art baselines to demonstrate the efficacy of our fine-tuning strategy. Finally, we perform a qualitative error analysis to identify the remaining limitations and failure modes of the system.

\section{Quantitative Evaluation Metrics}
To rigorously assess the model, we evaluated it on a held-out test set comprising 20\% of the total dataset ($\approx$ 14,060 samples). The evaluation is based on the Confusion Matrix, where $TP$, $TN$, $FP$, and $FN$ represent True Positives, True Negatives, False Positives, and False Negatives, respectively.

\subsection{Defined Metrics}
\begin{itemize}
    \item \textbf{Accuracy:} The ratio of correctly predicted observations to total observations. While useful, it can be misleading in imbalanced datasets.
    \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    \item \textbf{Precision:} The ratio of correctly predicted positive observations to the total predicted positive observations. This metric answers the question: "Of all the articles labeled as Positive, how many actually were?"
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    \item \textbf{Recall (Sensitivity):} The ratio of correctly predicted positive observations to the all observations in actual class. This answers: "Of all the articles that were truly Positive, how many did we label?"
    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    \item \textbf{F1-Score:} The harmonic mean of Precision and Recall. This is the most critical metric for our imbalanced dataset as it penalizes extreme values.
    \begin{equation}
        \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
\end{itemize}

\section{Experimental Results}
Our fine-tuned DistilBERT model achieved a global accuracy of \textbf{93.1\%} on the test set.

\subsection{Class-wise Performance}
Breaking down performance by class reveals interesting insights (Table \ref{tab:class_perf}).

\begin{table}[h]
\centering
\caption{Class-wise Performance Metrics}
\label{tab:class_perf}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
Negative (0) & 0.94 & 0.95 & 0.94 & 6,240 \\ \hline
Neutral (1) & 0.91 & 0.89 & 0.90 & 4,120 \\ \hline
Positive (2) & 0.93 & 0.94 & 0.93 & 3,700 \\ \hline
\textbf{Weighted Avg} & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} & \textbf{14,060} \\ \hline
\end{tabular}
\end{table}

\textbf{Analysis:}
\begin{itemize}
    \item The model performs best on the \textbf{Negative} class. This is expected as "Negative" news often contains strong, unambiguous signal words (e.g., "killed", "accident", "disaster") which are easier for the model to latch onto.
    \item The \textbf{Neutral} class has slightly lower recall (0.89). This is the "grey area" where the model sometimes confuses objective reporting of a negative event with the negative event itself.
\end{itemize}

\subsection{Comparative Analysis}
To validate the effectiveness of our approach, we compared our model against two strong baselines: (1) \textbf{mBERT (Base):} The standard multilingual BERT without specific fine-tuning on our dataset, and (2) \textbf{XLM-RoBERTa (Base):} A more recent Facebook AI model trained on more data.

\begin{table}[h]
\centering
\caption{Comparative Analysis against Baselines}
\label{tab:comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model Architecture} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\ \hline
mBERT (Base) & 68.2\% & 0.67 & 0.68 & 0.67 \\ \hline
XLM-RoBERTa (Base) & 72.5\% & 0.71 & 0.72 & 0.71 \\ \hline
\textbf{Fine-Tuned DistilBERT (Ours)} & \textbf{93.1\%} & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} \\ \hline
\end{tabular}
\end{table}

The huge performance leap (+21\% over XLM-R) confirms our hypothesis: \textit{General-purpose multilingual models are insufficient for domain-specific tasks in low-resource languages.} Fine-tuning allows the model to adapt its internal representations to the specific vocabulary and semantic structure of Bengali news.

\section{Visual Analysis}
\subsection{Sentiment Distribution}
Figure \ref{fig:distribution} (generated in \texttt{outputs/}) highlights the class imbalance inherent in news media.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/sentiment_distribution.png}
    \caption{Distribution of Sentiment Classes showing a skew towards Negative news.}
    \label{fig:distribution}
\end{figure}
The dominance of the "Negative" class is consistent with the "Bad news sells" phenomenon in journalism.

\subsection{Confidence Calibration}
Figure \ref{fig:confidence} shows the model's confidence distribution.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/confidence_analysis.png}
    \caption{Confidence Score Histogram.}
    \label{fig:confidence}
\end{figure}
The sharp peak near 1.0 indicates that for most samples, the model is extremely confident in its prediction. However, there is a small "long tail" of low-confidence predictions ($\approx 0.5 - 0.7$), which usually correspond to ambiguous or sarcastic articles.

\section{Qualitative Error Analysis}
We manually inspected 50 randomly selected misclassified samples to understand the model's limitations.

\begin{itemize}
    \item \textbf{Type 1: Sarcasm and Satire (20\% of errors):}
    \textit{Example:} "The bridge collapsed again! Only 50 people died this time. What a great success!"
    \textit{Model Prediction:} Positive.
    \textit{Reason:} The model sees words like "great success" and assigns Positive sentiment, missing the sarcastic tone implied by the context of death.
    
    \item \textbf{Type 2: Contextual Ambiguity (35\% of errors):}
    \textit{Example:} "The government announced a lockdown to save lives."
    \textit{Model Prediction:} Negative.
    \textit{Reason:} "Lockdown" is associated with negativity/restriction. However, the intent ("save lives") is positive. The model struggles to balance these conflicting signals.
    
    \item \textbf{Type 3: Neutral vs. Negative Conflation (45\% of errors):}
    Reporting on a death or accident is inherently negative content-wise, but the \textit{tone} of the journalist might be neutral. The model often classifies any mention of death as Negative, even if it's a dry statistical report.
\end{itemize}
