\chapter{Results and Critical Analysis}
\label{chap:Results}

\section{Introduction}
This chapter presents a comprehensive evaluation of the performance of the proposed system. We utilize standard statistical metrics to quantify the model's accuracy, precision, recall, and F1-score. Furthermore, we conduct a comparative analysis against existing state-of-the-art baselines to demonstrate the efficacy of our fine-tuning strategy. Finally, we perform a qualitative error analysis to identify the remaining limitations and failure modes of the system.

\section{Performance Evaluation}
After training the model for 5 epochs (roughly 4 hours on Colab), I ran it against the held-out test set of 14,060 images. The headline number is good: \textbf{93.1\% Accuracy}. But accuracy is a liar.
If I had just guessed "Negative" (the majority class) for every single article, I would still have achieved $\approx 45\%$ accuracy. So, accuracy alone doesn't tell me if the model is actually learning.

\subsection{Beyond Accuracy: The F1-Score}
To get the real picture, I looked at the **F1-Score**, specifically the weighted average.
\begin{itemize}
    \item \textbf{Why F1?} It balances Precision (Trustworthiness) and Recall (Coverage).
    \item \textbf{The Result:} My weighted F1-Score was \textbf{0.93}. This confirms that the high accuracy isn't a fluke; the model is genuinely performing well across all categories, not just the majority class.
\end{itemize}

\section{Detailed Results Breakdown}
Table \ref{tab:class_perf} shows the "Report Card" for the model.

\begin{table}[h]
\centering
\caption{Class-wise Performance Metrics}
\label{tab:class_perf}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
Negative (0) & 0.94 & 0.95 & 0.94 & 6,240 \\ \hline
Neutral (1) & 0.91 & 0.89 & 0.90 & 4,120 \\ \hline
Positive (2) & 0.93 & 0.94 & 0.93 & 3,700 \\ \hline
\textbf{Weighted Avg} & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} & \textbf{14,060} \\ \hline
\end{tabular}
\end{table}

\subsection{What does this tell us?}
\begin{enumerate}
    \item \textbf{Negative News is Easy:} The model scored highest (94\% F1) on Negative news. This makes sense. Words like "accident", "death", and "crash" are strong, unambiguous signals.
    \item \textbf{Neutral is Hard:} The Neutral class had the lowest recall (89\%). This is the "Grey Zone." The model often struggles to tell the difference between a \textit{neutral report about a bad event} (e.g., "5 people died in a crash") and the \textit{bad event itself}. To a human, the report is neutral; to the model, "died" equals negative.
\end{enumerate}

\subsection{Beating the Baselines}
I compared my fine-tuned model against the raw, off-the-shelf versions of mBERT and XLM-RoBERTa.
\begin{itemize}
    \item **mBERT (Base):** 68.2\% Accuracy.
    \item **XLM-RoBERTa:** 72.5\% Accuracy.
    \item **My Fine-Tuned DistilBERT:** 93.1\% Accuracy.
\end{itemize}
The 20\% jump proves that for Bengali, you cannot just take a pre-trained model and hope for the best. You \textit{must} fine-tune it on domain data.

\section{Visual Analysis}
\subsection{Sentiment Distribution}
Figure \ref{fig:distribution} (generated in \texttt{outputs/}) highlights the class imbalance inherent in news media.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/sentiment_distribution.png}
    \caption{Distribution of Sentiment Classes showing a skew towards Negative news.}
    \label{fig:distribution}
\end{figure}
The dominance of the "Negative" class is consistent with the "Bad news sells" phenomenon in journalism.

\subsection{Confidence Calibration}
Figure \ref{fig:confidence} shows the model's confidence distribution.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/confidence_analysis.png}
    \caption{Confidence Score Histogram.}
    \label{fig:confidence}
\end{figure}
The sharp peak near 1.0 indicates that for most samples, the model is extremely confident in its prediction. However, there is a small "long tail" of low-confidence predictions ($\approx 0.5 - 0.7$), which usually correspond to ambiguous or sarcastic articles.

\section{Where the Model Failed (Qualitative Analysis)}
Numbers don't tell the whole story. To understand the model's "mind," I manually read 50 articles where the model got it wrong. The errors weren't random; they fell into three distinct patterns.

\subsection{Failure Mode 1: The Sarcasm Blindspot}
Sarcasm is the arch-nemesis of AI.
\begin{itemize}
    \item \textbf{The Article:} A political column mocking a corrupt official saying, "The bridge collapsed again! Only 50 people died this time. What a great success!"
    \item \textbf{The Prediction:} \textbf{Positive} (98\% Confidence).
    \item \textbf{The Diagnosis:} The model saw "Great Success" and ignored the context. It doesn't understand irony. It takes everything literally.
\end{itemize}

\subsection{Failure Mode 2: The "Good Intent" Paradox}
The model struggles when a negative action is taken for a positive reason.
\begin{itemize}
    \item \textbf{The Article:} "The government announced a strict lockdown to save lives."
    \item \textbf{The Prediction:} \textbf{Negative}.
    \item \textbf{The Diagnosis:} "Lockdown" is a negative word in the dataset (associated with economic loss). The model missed the "save lives" part because the negative signal from "Lockdown" was too strong.
\end{itemize}

\subsection{Failure Mode 3: The "Tragic Neutral" Confusion}
This was the most common error (45\% of mistakes).
\begin{itemize}
    \item \textbf{The Article:} "A bus fell into a ditch. 3 people were injured."
    \item \textbf{The Prediction:} \textbf{Negative}.
    \item \textbf{The Truth:} The \textit{journalist's tone} is Neutral (just reporting facts). But the \textit{event} is Negative. The model can't separate the reporter's voice from the event's nature.
\end{itemize}
