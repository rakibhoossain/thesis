\chapter{Literature Review and Theoretical Background}
\label{chap:LitReview}

\section{Introduction}
This chapter provides the theoretical foundation necessary to understand the proposed sentiment analysis system. We begin by tracing the historical evolution of sentiment analysis, from early rule-based systems to the modern era of Deep Learning. We then provide an in-depth mathematical explanation of the Transformer architecture, which forms the backbone of our model. Finally, we critically review the state-of-the-art research in Bengali sentiment analysis, highlighting the specific gaps this thesis aims to address.

\section{The Evolution of Sentiment Analysis}
It is helpful to think of sentiment analysis not as a single technology, but as a story of three acts. Each act solved a specific problem but created a new one.

\subsection{Act I: The Dictionary Era}
In the early days, we treated sentiment as a simple accounting problem. If a sentence had more "positive" words than "negative" ones, it was positive. 
\begin{itemize}
    \item \textbf{The Approach:} Researchers built lexicons—massive lists where "good" = +1 and "bad" = -1. Tools like VADER \cite{hutto2014vader} became staples.
    \item \textbf{The Failure Point:} Language is rarely so binary. In Bengali, this approach collapsed entirely. Why? because many researchers simply translated English dictionaries into Bengali. A phrase like ``মাথামুন্ডু'' (Mathamundu) literally translates to "Head-Head," which sounds neutral. But to any Bengali speaker, it screams "Nonsense" (negative). Dictionaries couldn't capture this cultural context.
\end{itemize}

\subsection{Act II: The Statistical Era}
Realizing that words change meaning based on neighbors, the field moved to Machine Learning.
\begin{itemize}
    \item \textbf{The Approach:} We started feeding texts into algorithms like Support Vector Machines (SVM). Instead of predefined lists, the model "learned" that the word "slow" is bad in a laptop review but might be good in a generic "slow cooking" context.
    \item \textbf{The Failure Point:} These models were "shallow." They relied on us humans to tell them what to look for (Feature Engineering). For an inflected language like Bengali, this was a nightmare. A verb like "cor" (do) has dozens of variations. To an SVM, "cor" and "corchi" (doing) look like two completely unrelated words. The datasets grew too sparse to be useful (the "Curse of Dimensionality").
\end{itemize}

\subsection{Act III: The Deep Learning Era (Current)}
The current era began when we stopped telling computers \textit{how} to read and started letting them learn representations.
\begin{itemize}
    \item \textbf{The Shift:} Word Embeddings (like Word2Vec) allowed computers to understand that "King" - "Man" + "Woman" $\approx$ "Queen." 
    \item \textbf{The Limitation:} Even early Deep Learning (RNNs/LSTMs) had a flaw: they read sequentially, one word at a time. They would often "forget" the start of a long paragraph by the time they reached the end. This sequential processing also made them painfully slow to train.
\end{itemize}

\section{The Transformer: A Paradigm Shift}
In 2017, Vaswani et al. \cite{vaswani2017attention} proposed a radical idea: \textit{what if we stopped reading sentences left-to-right and read everything at once?} This was the birth of the Transformer.

\subsection{The "Spotlight" Mechanism (Self-Attention)}
The core breakthrough was ``Self-Attention.'' Imagine you are reading a complex sentence. When you see the word "bank," you don't know if it means a river bank or a financial bank until you look at the other words in the sentence (like "money" or "water"). 

Self-Attention acts like a spotlight. For every word, it scans the entire sentence to see which \textit{other} words shed light on its meaning.

Mathematically, this is implemented using three vectors for every word: a \textbf{Query} ($Q$), a \textbf{Key} ($K$), and a \textbf{Value} ($V$). Think of it like a library filing system:
\begin{itemize}
    \item \textbf{Query ($Q$):} What I am looking for? (e.g., "I am the word 'bank', what defines me?")
    \item \textbf{Key ($K$):} The label on the folder.
    \item \textbf{Value ($V$):} The actual content inside the folder.
\end{itemize}

The model calculates a score to decide how much "Attention" the Query should pay to the Key:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

If the dot product ($QK^T$) is high, the "Spotlight" shines brightly on that connection. The $\sqrt{d_k}$ term is simply a scaling factor to keep the math stable.

\subsection{Why Multiple Heads?}
One spotlight isn't enough. A word might need to focus on grammar ("is it a verb?") and semantic meaning ("is it happy?") simultaneously. This is where \textbf{Multi-Head Attention} comes in. It runs several attention layers in parallel, allowing the model to "look" at the sentence from multiple perspectives at once.

\subsection{Positional Encoding: Adding Order to Chaos}
Because the Transformer reads the whole sentence instantly (in parallel), it technically doesn't know that word \#1 comes before word \#2. To fix this, we inject \textbf{Positional Encodings}—mathematical timestamps (using Sine and Cosine waves)—so the model knows the order of words.
\begin{equation}
    PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
\end{equation}
Without this, "The dog bit the man" and "The man bit the dog" would look identical to the model.

\section{Masked Language Models: BERT and DistilBERT}
\subsection{BERT (Bidirectional Encoder Representations from Transformers)}
BERT \cite{devlin2018bert} utilizes the Encoder stack of the Transformer. It introduced the concept of contextual pre-training using two unsupervised tasks:
\begin{enumerate}
    \item \textbf{Masked Language Modeling (MLM):} Randomly masking 15\% of input tokens and asking the model to predict the missing word based on context. This forces the model to learn bidirectional context.
    \item \textbf{Next Sentence Prediction (NSP):} Predicting if Sentence B logically follows Sentence A, helping the model understand discourse relationships.
\end{enumerate}

\subsection{DistilBERT (Distilled BERT)}
While BERT achieved state-of-the-art results, it is computationally expensive (110M params for Base). Sanh et al. \cite{sanh2019distilbert} introduced DistilBERT, which applies \textit{Knowledge Distillation}. A smaller student model (DistilBERT) is trained to mimic the soft-target probabilities of the larger teacher model (BERT).
\begin{equation}
    L_{distill} = \sum p(\text{teacher}) \cdot \log p(\text{student})
\end{equation}
DistilBERT retains 97\% of BERT's performance while being 40\% smaller and 60\% faster, making it ideal for deployment in resource-constrained environments like ours.

\section{The Landscape of Bengali Sentiment Analysis}
Research in Bengali NLP has often felt like playing catch-up with English. Reviewing the works from 2020-2025 reveals a pattern of promising starts but consistent limitations.

\subsection{The "Hybrid" Trap}
A common trend has been to combine old and new methods. Rahman et al. (2024) \cite{rahman2024enhancing} proposed a "Hybrid" model that used VADER rules to "correct" a BanglaBERT model. Their hypothesis was that domain rules could fix deep learning errors. While innovative, their dataset was tiny ($<$5,000 samples). In my view, this approach patches a symptom rather than curing the disease—if the underlying model is trained on enough data, it shouldn't need manual rule-based corrections.

\subsection{The Data Fragmentation Problem}
The biggest hurdle isn't the model; it's the data.
\begin{itemize}
    \item **Informal Focus:** Khan et al. \cite{khan2024bangla} released "BanglaSenti" (35k samples). It's a great dataset, but it's scraped from Facebook comments. A model trained on comments like "fatafati" (awesome) fails when reading a formal editorial about inflation.
    \item **Closed Doors:** Some promising works, like the \textit{skipBangla-BERT} by Islam et al. \cite{islam2024bangdsa}, claim high accuracy but never released their code or data. Science that isn't reproducible is of limited use to the community.
\end{itemize}

\subsection{Gap Analysis: What's Missing?}
This thesis exists because of three specific holes in the current literature:
\begin{enumerate}
    \item \textbf{No Formal News Dataset:} There is no open-source dataset of 50k+ \textit{formal, full-length} Bengali news articles labeled for sentiment.
    \item \textbf{Usability Gap:} Researchers build models but rarely build tools. A Graph in a PDF doesn't help a journalist; a web-app does.
    \item \textbf{Efficiency:} Most benchmarks use "Large" models that require expensive GPUs. We need models that work on standard hardware.
\end{enumerate}

This thesis directly targets these gaps by creating a large-scale news dataset (70k+), fine-tuning a lightweight yet powerful model (DistilBERT), and deploying a full web-based analysis suite.